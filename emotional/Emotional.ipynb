{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e035ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from snownlp import SnowNLP\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import jieba\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91907e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接跑jms的csv\n",
    "# readpath = r'data/processing/'\n",
    "# readpath = r'./rationality/'\n",
    "# dataframe = pd.read_csv(readpath+'precessed.csv')\n",
    "# dataframe = dataframe.head()\n",
    "# dataframe = pd.read_csv(readpath+'processed(ver3.0).csv')\n",
    "dataframe = pd.read_csv('precessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd8221",
   "metadata": {},
   "source": [
    "## 函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回的是negative的概率；positive的概率就是1-negative\n",
    "# def get_sentiment(text):\n",
    "#     model_name_or_path = \"./local_model\"\n",
    "#     # 加载分词器\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "#     # 加载模型\n",
    "#     model = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "#     # tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "#     # model = TFBertForSequenceClassification.from_pretrained(model_name_or_path, from_pt=True)\n",
    "#     # tokenizer = BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment')\n",
    "#     # model = TFBertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment', from_pt=True)\n",
    "\n",
    "#     inputs = tokenizer(text, return_tensors='tf')\n",
    "#     output = model(**inputs)\n",
    "\n",
    "#     probs = tf.nn.softmax(output.logits, axis=-1)\n",
    "#     probs_np = probs.numpy()  # Convert to numpy array\n",
    "\n",
    "#     prob_class1 = probs_np[0, 0]  # Probability of the first class\n",
    "#     prob_class2 = probs_np[0, 1]  # Probability of the second class\n",
    "\n",
    "#     return prob_class1\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name_or_path = \"./local_model\"\n",
    "# 加载分词器\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# # 加载模型\n",
    "# model = AutoModel.from_pretrained(model_name_or_path)\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    negative_prob = probs[0][0].item()  # 假设第一个标签是负面情感\n",
    "    return negative_prob\n",
    "\n",
    "# 情感获取\n",
    "def sentiment(list):\n",
    "    negative_prob = []\n",
    "    for item in list:\n",
    "        if isinstance(item, str):\n",
    "            negative = get_sentiment(item)\n",
    "            negative_prob.append(negative)\n",
    "        else:\n",
    "            negative_prob.append(None)\n",
    "    return negative_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8f674f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标题文本处理\n",
    "def process_title(data):\n",
    "    titles = []\n",
    "    for title in data:\n",
    "        if isinstance(title, str):\n",
    "            if title[:4] == '慈善募捐':\n",
    "                titles.append(title[7:-7])\n",
    "            else:titles.append(title)\n",
    "        else:\n",
    "            titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35210a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#项目介绍文本处理\n",
    "def process_detail(data):\n",
    "\n",
    "    patterns = [r'（[^（）]*?图[^（）]*?）', r'【[^【】]*?图[^【】]*?】', r'（[^（）]*?照片[^（）]*?）', r'【[^【】]*?照片[^【】]*?】']\n",
    "    photo = [] # 照片数量\n",
    "    detail_ = [] # 剔除这类信息后的文本\n",
    "    details = [] # 最终返回的文本\n",
    "\n",
    "    for detail in data:\n",
    "        if isinstance(detail, str):\n",
    "            match = []\n",
    "            for pattern in patterns:\n",
    "                match = match + re.findall(pattern, detail, flags=0)\n",
    "                detail = re.sub(pattern, '', detail)\n",
    "            photo.append(len(match))\n",
    "            detail_.append(detail)\n",
    "        else:\n",
    "            detail_.append('')\n",
    "            photo.append(0)\n",
    "\n",
    "    for detail in detail_:\n",
    "        if detail != '':\n",
    "            text = ''.join(detail.split(r\"', '\"))\n",
    "            details.append(text)\n",
    "        else:\n",
    "            details.append(detail)\n",
    "\n",
    "\n",
    "    return photo, details\n",
    "\n",
    "\n",
    "# jieba分词，返回分出来的词，同时剔除非中文非数字字符；处理对象为文本\n",
    "def clean(text):\n",
    "\n",
    "    pattern = r'[^\\u4e00-\\u9fa5\\d]' #用于剔除非中文非数字字符\n",
    "    clean_words = []\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        words = jieba.lcut(text)\n",
    "\n",
    "        for word in words:\n",
    "            clean_word = re.sub(r'[^\\u4e00-\\u9fa5\\d]', '', word)\n",
    "            if clean_word != '':\n",
    "                clean_words.append(clean_word)\n",
    "\n",
    "    return clean_words\n",
    "\n",
    "\n",
    "# 信息量，计算clean后分词的数量\n",
    "def get_info(lst):\n",
    "\n",
    "    info = []\n",
    "\n",
    "    for item in lst:\n",
    "        clean_words = clean(item)\n",
    "        info.append(len(clean_words))\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "# 有强烈情感的符号\n",
    "def get_mark(data):\n",
    "\n",
    "    Exclamation = []\n",
    "    QMark = []\n",
    "\n",
    "    for text in data:\n",
    "        words = jieba.lcut(text)\n",
    "        Exclamation.append(words.count('？')+words.count('?'))\n",
    "        QMark.append(words.count('！'))\n",
    "\n",
    "    return Exclamation, QMark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35be9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算entropy\n",
    "def document_frequency(list_of_text):\n",
    "\n",
    "    # Initialize the document frequency dictionary\n",
    "    document_frequency = defaultdict(int)\n",
    "\n",
    "    # Iterate over each text and update the document frequency dictionary\n",
    "    for text in list_of_text:\n",
    "        if isinstance(text, str):\n",
    "            words = jieba.lcut(text)\n",
    "            for word in words:\n",
    "                document_frequency[word] += 1\n",
    "    return document_frequency\n",
    "\n",
    "\n",
    "def calculate_entropy(text, list_of_text):\n",
    "    # 需要先有document_frequency，可以用上一个函数获取。\n",
    "    # 返回的是输入内容的entropy\n",
    "    if isinstance(text, str):\n",
    "        words = jieba.lcut(text)\n",
    "        word_counts = Counter(words)\n",
    "        total_words = len(words)\n",
    "        entropy = 0.0\n",
    "\n",
    "        for word, count in word_counts.items():\n",
    "            probability = count / total_words\n",
    "            inverse_document_frequency = math.log((len(list_of_text) + 1) / (document_frequency[word] + 1))\n",
    "            entropy += inverse_document_frequency * probability * math.log(probability, 2)\n",
    "\n",
    "        entropy = -entropy\n",
    "    else:\n",
    "        entropy = None\n",
    "    return entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f954251",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70d3cc",
   "metadata": {},
   "source": [
    "### 一起跑（后面的就不用跑了）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b354cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title\n",
    "titles = process_title(dataframe['项目名称'].tolist())\n",
    "TInfo = get_info(titles)\n",
    "TEmo = sentiment(titles)\n",
    "dataframe['TInfo'] = TInfo\n",
    "dataframe['TEmo'] = TEmo\n",
    "\n",
    "# Brief\n",
    "briefs = dataframe['项目简介'].tolist()\n",
    "BInfo = get_info(briefs)\n",
    "BEmo = sentiment(briefs)\n",
    "dataframe['BInfo'] = BInfo\n",
    "dataframe['BEmo'] = BEmo\n",
    "\n",
    "# Detail\n",
    "photo, details = process_detail(dataframe['项目介绍'].tolist())\n",
    "DInfo = get_info(details)\n",
    "Exclamation, QMark = get_mark(details)\n",
    "\n",
    "detail_short = [detail[:500] for detail in details]\n",
    "DEmo = sentiment(detail_short)\n",
    "\n",
    "document_frequency = document_frequency(details)\n",
    "entropy = [calculate_entropy(detail,details) for detail in details]\n",
    "\n",
    "dataframe['photo'] = photo\n",
    "dataframe['DInfo'] = DInfo\n",
    "dataframe['DEmo'] = DEmo\n",
    "dataframe['Exclamation'] = Exclamation\n",
    "dataframe['QMark'] = QMark\n",
    "dataframe['entropy'] = entropy\n",
    "\n",
    "# 数据存到新的csv里面\n",
    "# dataframe.to_csv(readpath + r\"processed(ver3.0).csv\", index=False)\n",
    "dataframe.to_csv(r\"precessed_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e671e44",
   "metadata": {},
   "source": [
    "### 标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "briefs = dataframe['项目简介'].tolist()\n",
    "BInfo = get_info(briefs)\n",
    "dataframe['BInfo'] = BInfo\n",
    "BEmo = sentiment(briefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a465c8",
   "metadata": {},
   "source": [
    "### 项目简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19540f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = process_title(dataframe['项目名称'].tolist())\n",
    "TInfo = get_info(titles)\n",
    "TEmo = sentiment(titles)\n",
    "dataframe['TInfo'] = TInfo\n",
    "dataframe['TEmo'] = TEmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d82bca",
   "metadata": {},
   "source": [
    "### 项目介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "photo, details = process_detail(dataframe['项目介绍'].tolist())\n",
    "DInfo = get_info(details)\n",
    "Exclamation, QMark = get_mark(details)\n",
    "document_frequency = document_frequency(details)\n",
    "entropy = [calculate_entropy(detail,details) for detail in details]\n",
    "dataframe['entropy'] = entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5440fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(readpath + r\"processed(ver4.0).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2178a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
